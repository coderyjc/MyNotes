## 缺失值处理

### 寻找缺失值

count()函数可以统计不为空数据的个数
shape()函数则可以统计数据样本的个数

```python
(user_info.shape[0]-user_info['age_range'].count()) /user_info.shape[0]

# 输出为0.00522667798288422
```


### 缺失值处理

Pandas包中包含了一些处理缺失值的函数

| 方法    | 说明                                                                             |
| ------- | -------------------------------------------------------------------------------- |
| dropna  | 根据各标签中的值是否存在缺失数据对轴标签进行过滤，可通过阔值调节对缺失值的容忍度 |
| fillna  | 用指定值或插值方法（如ffill或bfil1）填充缺失值                                   |
| isnull  | 返回一个含有bool型的对象，这些bool型的值表示哪些是缺失值NaN                      |
| notnull | isnull的否定式                                                                   |

缺失值较少时进行填充
数据缺失特别严重的数据，一般将其删除处理

数据填充方法

| 处理方法   | 说明                                                                                                       |
| ---------- | ---------------------------------------------------------------------------------------------------------- |
| 统计量填充 | 连续值：推荐使用中位数，排除一些特别大或特别小的异常值造成的影响；离散值：推荐使用众数，不能用均值和中位数 |
| 特殊值填充 | 填一个不在正常取值范围内的数值，如使用-999，0等表示缺失值                                                  |
| 不处理     | XGB（XGBoost）和LGB（LightGBM）这两种比赛常用的预测模型对缺失值并不敏感，**算法本身有一套缺失值处理算法**      |

常用的插值方法说明

| 插补方法     | 说明                                                     | 优点                             | 缺点                                   | 使用环境       |
| ------------ | -------------------------------------------------------- | -------------------------------- | -------------------------------------- | -------------- |
| 类均值插补   | 数值型：均值 ；非数值型：众数（出现频率最高的值）        | 简单稳定                         | 不能反映缺失值的变异性，低估资料变异   | 低缺失率首选   |
| 类随机插补   | 聚类填充：使用所有可能的值填充：组合完整化方法           | 能体现数据变异性                 | 依赖于观测值                           | 低缺失率       |
| 回归插补     | 基于完整的数据集，建立回归方程                           | 方差估计好                       | 稳定性依赖于辅助变量，抽样误差不易控制 | 变量间的相关强 |
| Em插补       | 通过观测数据的边际分布可以对未知参数进行极大似然估计     | 利用充分、考虑了缺失值的不稳定性 | 计算复杂                               | 高缺失率       |
| 多重插补MCMC | 估计出待插补的值，然后加上不同的噪声，形成多组可选插补值 | 利用充分、考虑了缺失值的不稳定性 | 计算复杂                               | 高缺失率首选   |

### 不均衡样本

```ad-question
title: 不均衡样本带来的影响（ChatGPT）

1. **偏向于多数类别：** 模型在训练过程中可能倾向于学习多数类别，因为多数类别的样本更多。这会导致模型对于少数类别的学习不足，难以正确预测少数类别的样本。
2. **评估指标失真：** 常用的评估指标如准确率可能不再是有效的性能度量，因为模型可能只是简单地预测为多数类别以达到较高的准确率。在不均衡数据中，需要使用其他评估指标，如精确度、召回率、F1分数等，来更全面地评估模型性能。
3. **过拟合多数类别：** 模型可能过度关注多数类别的样本，导致对于少数类别的泛化性能较差。模型可能会在多数类别上表现良好，但在少数类别上表现不足。
4. **决策边界偏移：** 模型的决策边界可能倾向于多数类别，使其对于少数类别的判定边界相对较远，造成对少数类别的错误分类。
5. **概念漂移：** 不均衡的样本分布可能导致模型在面对新数据时性能下降，因为新数据的分布可能与训练数据的分布不同。
```

常见处理方式：

1. 随机欠采样

比如正样本50例，负样本1000例，可以在负样本中随机抽样10%，与正样本组成新的训练集：100+50，这样正样本的比例就增加了。

优点是在平衡数据的同时减小了数据量，加速了训练；
缺点是数据减少会影响模型的特征学习能力和泛化能力。

2. 随机过采样

将正样本复制10份，和负样本一起组成训练集：1000+500=1500，这样，正样本所占的比例就大了。

优点是相对于欠采样，其没有导致数据信息的损失
缺点是对较少类别的复制增加了过拟合的可能性。

3. 基于聚类的过采样方法

基于聚类的过采样方法则通过先对整个数据集进行聚类，然后在聚类中对少数类别进行过采样。

优点：有助于增加少数类别的样本数量，使得模型更好地学习较少类别的特征，从而提高对这些类别的预测性能。
缺点：和一般的过采样方法一样，该方法容易使模型对训练数据过拟合。

4. SMOTE算法

Synthetic Minority Oversampling Technique，合成少数类过采样技术。基于随机过采样的一种改进方法。

SMOTE算法采用对少数样本进行人工合成的方式将新样本加入训练集中，从而使模型训练更具有泛化能力。
过程：
1. 算少量类别的样本$x_i$与所有样本点的距离。
2. 根据样本的不平衡设定过采样倍率，找到最近的k个样本。
3. 对于每一个新生成的样本按$x_{newj}=x_i+rand(0,1)*(x_j-x_i)$生成新的$x_j$

优点是通过人造相似样本取代直接复制的方法减弱了过拟合，也没有丢失有用的信息；
缺点是在进行人工合成样本时，由于没有考虑到近邻样本可能来自不同类别，因此导致增大类别间的重叠。

5. 基于数据清洗的SMOTE算法

实际数据中不但存在样本不均衡问题，还有不同类别的样本重叠问题。（图a）

如果直接进行SMOTE过采样，得到结果如图b所示，样本重叠加剧，不利于分类。

此时可以进行数据清洗，除掉重叠样本。即Tomk Link方法。

对于一对样本$(x_i,x_j)$，其中$x_i$来自minority classes，$x_j$来自majority classes。如果离$x_i$最近的少量类别为$x_j$，离$x_j$最近的少量类别为$x_i$，那么则称$(x_i,x_j)$是一个Tomek Link，如图2-2-3（c）中的虚线矩形框中的样本对。

当得到Tomek Link之后，移去所有的Tomek Link并且反复进行，会得到如图2-2-3（d）所示的结果。此时样本的重叠大大减小，从而有利于分类。

![[assets/Pasted image 20240124160302.png]]

其中星号是新增数据。

### 常见数据分布

离散分布：伯努利分布（Bernoulli Distribution）、二项分布（Binomial Distribution）、泊松分布（Poisson Distribution）、几何分布（Geometric Distribution）

连续分布：正态分布（Normal Distribution）、指数分布（Exponential Distribution）、β分布（Beta Distribution）