---
doc_type: weread-highlights-reviews
bookId: "30179184"
author: 雷姆兹·H.阿帕希杜塞尔 安德莉亚·C.阿帕希杜塞尔
cover: https://weread-1258476243.file.myqcloud.com/weread/cover/71/YueWen_30179184/t7_YueWen_30179184.jpg
reviewCount: 4
noteCount: 98
isbn: 9787115508232
category: 计算机-计算机综合
---

#读书/计算机-计算机综合

# 元数据
> [!bookinfo]+ 操作系统导论
> - ![ 操作系统导论|200](https://weread-1258476243.file.myqcloud.com/weread/cover/71/YueWen_30179184/t7_YueWen_30179184.jpg)
> - 书名： 操作系统导论
> - 作者： 雷姆兹·H.阿帕希杜塞尔 安德莉亚·C.阿帕希杜塞尔
> - 出版时间 2019-06-01 00:00:00
> - ISBN： 9787115508232
> - 分类： 计算机-计算机综合
> - 出版社： 人民邮电出版社

> [!bookinfo]+ 简介
> 这是一本关于现代操作系统的书。全书围绕虚拟化、并发和持久性这3个主要概念展开，介绍了所有现代系统的主要组件（包括调度、虚拟内存管理、磁盘和I/O子系统、文件系统 ）。 本书共50章，分为3个部分，分别讲述虚拟化、并发和持久性的相关内容。本书大部分章节均先提出特定的问题，然后通过书中介绍的技术、算法和思想来解决这些问题。笔者以对话形式引入所介绍的主题概念，行文诙谐幽默却又鞭辟入里，力求帮助读者理解操作系统中虚拟化、并发和持久性的原理。 本书内容全面，并给出了真实可运行的代码（而非伪代码），还提供了相应的练习，适合高等院校相关专业教师教学和高校学生自学。
# 高亮划线

## 第2章 操作系统介绍


- 📌 每个进程访问自己的私有虚拟地址空间（virtual address space）（有时称为地址空间，address space），操作系统以某种方式映射到机器的物理内存上。 ^9-6046-6132
    - ⏱ 2022-03-07 00:14:37 
## 第4章 抽象：进程


- 📌 操作系统通过虚拟化（virtualizing）CPU来提供这种假象。通过让一个进程只运行一个时间片，然后切换到其他进程，操作系统提供了存在多个虚拟CPU的假象。这就是时分共享（time sharing）CPU技术， ^11-789-896
    - ⏱ 2023-02-23 15:22:28 

- 📌 我们将低级机制称为机制（mechanism）。机制是一些低级方法或协议，实现了所需的功能。 ^11-1000-1045
    - ⏱ 2023-02-23 15:24:06 

- 📌 策略是在操作系统内做出某种决定的算法。 ^11-1477-1496
    - ⏱ 2023-02-23 15:24:12 

- 📌 机器状态（machine state）：程序在运行时可以读取或更新的内容。 ^11-1877-1914
    - ⏱ 2023-02-23 15:30:06 

- 📌 将程序和静态数据加载到内存中的过程，需要操作系统从磁盘读取这些字节，并将它们放在内存中的某处 ^11-3724-3770
    - ⏱ 2023-02-23 15:40:54 

- 📌 在早期的（或简单的）操作系统中，加载过程尽早（eagerly）完成，即在运行程序之前全部完成。现代操作系统惰性（lazily）执行该过程，即仅在程序执行期间需要加载的代码或数据片段，才会加载。 ^11-4017-4113
    - ⏱ 2023-02-23 15:44:58 

- 📌 为程序的运行时栈（run-time stack或stack）分配一些内存。 ^11-4275-4312
    - ⏱ 2023-02-23 15:46:21 

- 📌 UNIX系统中，默认情况下每个进程都有3个打开的文件描述符（file descriptor），用于标准输入、输出和错误 ^11-4658-4717
    - ⏱ 2023-02-23 16:04:15 

- 📌 一个常见的例子是，当进程向磁盘发起I/O请求时，它会被阻塞，因此其他进程可以使用处理器。 ^11-5509-5553
    - ⏱ 2023-02-23 16:11:48 
 

- 📌 有时候人们会将存储关于进程的信息的个体结构称为进程控制块（Process Control Block，PCB） ^11-9386-9441
    - ⏱ 2023-02-23 17:40:34 
## 第5章 插叙：进程API


- 📌 UNIX系统采用了一种非常有趣的创建新进程的方式，即通过一对系统调用：fork()和exec()。进程还可以通过第三个系统调用wait()，来等待其创建的子进程执行完成。 ^12-583-668
    - ⏱ 2023-02-23 17:42:28 

- 📌 进程调用了fork()系统调用，这是操作系统提供的创建新进程的方法。新创建的进程几乎与调用进程完全一样，对操作系统来说，这时看起来有两个完全一样的p1程序在运行，并都从fork()系统调用中返回。新创建的进程称为子进程（child），原来的进程称为父进程（parent）。子进程不会从main()函数开始执行（因此hello world信息只输出了一次），而是直接从fork()系统调用返回，就好像是它自己调用了fork()。 ^12-2171-2384
    - ⏱ 2023-02-23 17:51:26 

- 📌 子进程并不是完全拷贝了父进程。具体来说，虽然它拥有自己的地址空间（即拥有自己的私有内存）、寄存器、程序计数器等，但是它从fork()返回的值是不同的。父进程获得的返回值是新创建子进程的PID，而子进程获得的返回值是0。 ^12-2406-2515
    - ⏱ 2023-02-24 08:50:10 

- 📌 CPU调度程序（scheduler）决定了某个时刻哪个进程被执行 ^12-2879-2911
    - ⏱ 2023-02-23 17:52:43 

- 📌 有时候父进程需要等待子进程执行完毕，这很有用。这项任务由wait()系统调用（或者更完整的兄弟接口waitpid()） ^12-3200-3259
    - ⏱ 2023-02-23 17:54:44 

- 📌 如果父进程碰巧先运行，它会马上调用wait()。该系统调用会在子进程运行结束后才返回 ^12-4504-4581
    - ⏱ 2023-02-23 18:09:17 

- 📌 exec()会从可执行程序中加载代码和静态数据，并用它覆写自己的代码段（以及静态数据），堆、栈及其他内存空间也会被重新初始化。然后操作系统就执行该程序，将参数通过argv传递给该进程 ^12-6472-6563
    - ⏱ 2023-02-23 18:12:12 

- 📌 事实证明，这种分离fork()及exec()的做法在构建UNIX shell的时候非常有用，因为这给了shell在fork之后exec之前运行代码的机会，这些代码可以在运行新程序前改变环境，从而让一系列有趣的功能很容易实现。 ^12-6795-6907
    - ⏱ 2023-02-23 18:12:47 

- 📌 shell实现结果重定向的方式也很简单，当完成子进程的创建后，shell在调用exec()之前先关闭了标准输出（standard output），打开了文件newfile.txt。这样，即将运行的程序wc的输出结果就被发送到该文件，而不是打印在屏幕上。 ^12-7673-7799
    - ⏱ 2023-02-23 18:14:57 

- 📌 p4确实调用了fork来创建新的子进程，之后调用execvp()来执行wc。屏幕上没有看到输出，是由于结果被重定向到文件p4.output。 ^12-9358-9428
    - ⏱ 2023-02-23 18:15:42 

- 📌 UNIX管道也是用类似的方式实现的，但用的是pipe()系统调用。在这种情况下，一个进程的输出被链接到了一个内核管道（pipe）上（队列），另一个进程的输入也被连接到了同一个管道上。 ^12-9478-9569
    - ⏱ 2023-02-23 18:16:04 

- 📌 阅读man手册可以避免尴尬。当你询问同事某个fork细节时，他可能会回复：“RTFM”。这是他在有礼貌地督促你阅读man手册（Read the Man）。RTFM中的F只是为这个短语增加了一点色彩…… ^12-10110-10210
    - ⏱ 2023-02-23 18:17:10 
## 第6章 机制：受限直接执行


- 📌 基本思想很简单：运行一个进程一段时间，然后运行另一个进程，如此轮换。通过以这种方式时分共享（time sharing）CPU，就实现了虚拟化 ^13-422-492
    - ⏱ 2023-02-24 09:11:36 

- 📌 第一个是性能：如何在不增加系统开销的情况下实现虚拟化？第二个是控制权：如何有效地运行进程，同时保留对CPU的控制？ ^13-528-585
    - ⏱ 2023-02-24 09:20:02 

- 📌 我们采用的方法是引入一种新的处理器模式，称为用户模式（user mode）。在用户模式下运行的代码会受到限制。例如，在用户模式下运行时，进程不能发出I/O请求。这样做会导致处理器引发异常，操作系统可能会终止进程。与用户模式不同的内核模式（kernel mode），操作系统（或内核）就以这种模式运行。在此模式下，运行的代码可以做它喜欢的事，包括特权操作，如发出I/O请求和执行所有类型的受限指令。 ^13-2548-2759
    - ⏱ 2023-02-24 09:24:59 

- 📌 要执行系统调用，程序必须执行特殊的陷阱（trap）指令。该指令同时跳入内核并将特权级别提升到内核模式。一旦进入内核，系统就可以执行任何需要的特权操作（如果允许），从而为调用进程执行所需的工作。完成后，操作系统调用一个特殊的从陷阱返回（return-from-trap）指令，如你期望的那样，该指令返回到发起调用的用户程序中，同时将特权级别降低，回到用户模式。 ^13-3011-3190
    - ⏱ 2023-02-24 09:26:36 

- 📌 因此，C库中进行系统调用的部分是用汇编手工编码的，因为它们需要仔细遵循约定，以便正确处理参数和返回值，以及执行硬件特定的陷阱指令。现在你知道为什么你自己不必写汇编代码来陷入操作系统了，因为有人已经为你写了这些汇编。 ^13-3794-3901
    - ⏱ 2023-02-24 09:30:15 

- 📌 请注意，硬件在发生中断时有一定的责任，尤其是在中断发生时，要为正在运行的程序保存足够的状态，以便随后从陷阱返回指令能够正确恢复正在运行的程序。这一组操作与硬件在显式系统调用陷入内核时的行为非常相似，其中各种寄存器因此被保存（进入内核栈），因此从陷阱返回指令可以容易地恢复。 ^13-7445-7581
    - ⏱ 2023-02-24 10:03:20 

- 📌 重启（或在通常意义上说，重新开始运行一些软件）可能是构建强大系统的一个非常有用的工具[C+04]。具体来说，重新启动很有用，因为它让软件回到已知的状态，很可能是经过更多测试的状态。重新启动还可以回收旧的或泄露的资源（例如内存），否则这些资源可能很难处理。最后，重启很容易自动化。由于所有这些原因，在大规模集群互联网服务中，系统管理软件定期重启一些机器，重置它们并因此获得以上好处，这并不少见。 ^13-11606-11817
    - ⏱ 2023-02-24 10:17:03 
## 第7章 进程调度：介绍


- 📌 这个问题通常被称为护航效应（convoy effect）[B+79]，一些耗时较少的潜在资源消费者被排在重量级的资源消费者之后。 ^14-3584-3648
    - ⏱ 2023-02-24 10:30:19 

- 📌 几乎所有现代化的调度程序都是抢占式的（preemptive），非常愿意停止一个进程以运行另一个进程。 ^14-4925-4975
    - ⏱ 2023-02-24 10:33:12 

- 📌 响应时间定义为从任务到达系统到首次运行的时间。更正式的定义是：　　T响应时间= T首次运行−T到达时间 ^14-6788-6933
    - ⏱ 2023-02-24 12:59:56 

- 📌 时间片太短是有问题的：突然上下文切换的成本将影响整体性能。因此，系统设计者需要权衡时间片的长度，使其足够长，以便摊销（amortize）上下文切换成本，而又不会使系统不及时响应。 ^14-8298-8387
    - ⏱ 2023-02-24 13:01:11 

- 📌 上下文切换的成本不仅仅来自保存和恢复少量寄存器的操作系统操作。程序运行时，它们在CPU高速缓存、TLB、分支预测器和其他片上硬件中建立了大量的状态。切换到另一个工作会导致此状态被刷新，且与当前运行的作业相关的新状态被引入，这可能导致显著的性能成本 ^14-8679-8802
    - ⏱ 2023-02-24 13:01:38 

- 📌 如果你愿意不公平，你可以运行较短的工作直到完成，但是要以响应时间为代价。如果你重视公平性，则响应时间会较短，但会以周转时间为代价。这种权衡在系统中很常见。你不能既拥有你的蛋糕，又吃它[3]。 ^14-9195-9335
    - ⏱ 2023-02-24 14:16:56 

- 📌 关于缓存性能如何受上下文切换影响的一项很好的研究。在今天的系统中问题比较小，如今处理器每秒钟发出数十亿条指令，但上下文切换仍发生在毫秒的时间级别。 ^14-13287-13360
    - ⏱ 2023-02-24 14:22:39 
## 第8章 调度：多级反馈队列


- 📌 多级反馈队列是用历史经验预测未来的一个典型的例子，操作系统中有很多地方采用了这种技术（同样存在于计算机科学领域的很多其他地方，比如硬件的分支预测及缓存算法）。 ^15-1057-1136
    - ⏱ 2023-02-24 14:25:13 
 

- 📌 至此，我们得到了MLFQ的两条基本规则。● 规则1：如果A的优先级 > B的优先级，运行A（不运行B）。● 规则2：如果A的优先级 = B的优先级，轮转运行A和B。 ^15-1768-1949
    - ⏱ 2023-02-24 14:26:54 

- 📌 ● 规则3：工作进入系统时，放在最高优先级（最上层队列）。● 规则4a：工作用完整个时间片后，降低其优先级（移入下一个队列）。● 规则4b：如果工作在其时间片以内主动释放CPU，则优先级不变。 ^15-2668-2877
    - ⏱ 2023-02-24 14:28:21 

- 📌 如果不知道工作是短工作还是长工作，那么就在开始的时候假设其是短工作，并赋予最高优先级。如果确实是短工作，则很快会执行完毕，否则将被慢慢移入低优先级队列，而这时该工作也被认为是长工作了。通过这种方式，MLFQ近似于SJF。 ^15-3794-3904
    - ⏱ 2023-02-24 14:30:30 

- 📌 假设交互型工作中有大量的I/O操作（比如等待用户的键盘或鼠标输入），它会在时间片用完之前放弃CPU。在这种情况下，我们不想处罚它，只是保持它的优先级不变。 ^15-4048-4125
    - ⏱ 2023-02-25 08:56:10 

- 📌 ● 规则5：经过一段时间S，就将系统中所有工作重新加入最高优先级队列。 ^15-5600-5661
    - ⏱ 2023-02-24 14:34:46 

- 📌 新规则一下解决了两个问题。首先，进程不会饿死——在最高优先级队列中，它会以轮转的方式，与其他高优先级工作分享CPU，从而最终获得执行。其次，如果一个CPU密集型工作变成了交互型，当它优先级提升时，调度程序会正确对待它。 ^15-5674-5783
    - ⏱ 2023-02-25 09:00:09 

- 📌 规则4：一旦工作用完了其在某一层中的时间配额（无论中间主动放弃了多少次CPU），就降低其优先级（移入低一级队列）。 ^15-6738-6804
    - ⏱ 2023-02-24 14:48:37 

- 📌 因此，许多系统使用某种类型的MLFQ作为自己的基础调度程序，包括类BSD UNIX系统[LM+89，B86]、Solaris[M06]以及Windows NT和其后的Window系列操作系统。 ^15-9851-9947
    - ⏱ 2023-02-24 14:53:06 
## 第9章 调度：比例份额


- 📌 基本思想很简单：每隔一段时间，都会举行一次彩票抽奖，以确定接下来应该运行哪个进程。越是应该频繁运行的进程，越是应该拥有更多地赢得彩票的机会。 ^16-620-690
    - ⏱ 2023-02-24 14:54:42 

- 📌 客户端可以将自己的彩票转让给服务端，从而尽可能加速服务端执行自己请求的速度。服务端执行结束后会将这部分彩票归还给客户端。 ^16-3280-3340
    - ⏱ 2023-02-24 15:00:46 

- 📌 将列表项按照彩票数递减排序。这个顺序并不会影响算法的正确性，但能保证用最小的迭代次数找到需要的节点，尤其当大多数彩票被少数进程掌握时。 ^16-5063-5130
    - ⏱ 2023-02-24 19:02:19 

- 📌 基本思路很简单：当需要进行调度时，选择目前拥有最小行程值的进程，并且在运行之后将该进程的行程值增加一个步长。 ^16-6680-6734
    - ⏱ 2023-02-24 19:10:19 

- 📌 彩票调度算法只能一段时间后，在概率上实现比例，而步长调度算法可以在每个调度周期后做到完全正确。 ^16-7652-7699
    - ⏱ 2023-02-24 19:11:12 

- 📌 彩票调度算法能够更合理地处理新加入的进程 ^16-7885-7905
    - ⏱ 2023-02-24 19:11:54 
## 第11章 关于CPU虚拟化的总结对话


- 📌 首先，我了解了操作系统如何虚拟化CPU。为了理解这一点，我必须了解一些重要的机制（mechanism）：陷阱和陷阱处理程序，时钟中断以及操作系统和硬件在进程间切换时如何谨慎地保存和恢复状态。 ^18-598-693
    - ⏱ 2023-02-24 19:13:52 
## 第13章 抽象：地址空间


- 📌 ￼图13.3　地址空间的例子 ^20-3039-3083
    - ⏱ 2023-02-24 19:24:35 

- 📌 隔离是建立可靠系统的关键原则。 ^20-3998-4013
    - ⏱ 2023-02-24 19:28:26 

- 📌 你也可以打印出来，是的，如果你可以打印它，它也是一个虚拟地址。实际上，作为用户级程序的程序员，可以看到的任何地址都是虚拟地址。只有操作系统，通过精妙的虚拟化内存技术，知道这些指令和数据所在的物理内存的位置 ^20-5156-5258
    - ⏱ 2023-02-24 19:37:38 

- 📌 并且将由操作系统和硬件翻译成物理地址，以便从真实的物理位置获取该地址的值 ^20-5997-6033
    - ⏱ 2023-02-24 20:03:10 
## 第14章 插叙：内存操作API


- 📌 sizeof() 被正确地认为是一个操作符，而不是一个函数调用（函数调用在运行时发生）。 ^21-2574-2618
    - ⏱ 2023-02-24 20:03:11 

- 📌 在这种情况下，编译器有足够的静态信息，知道已经分配了40个字节。 ^21-3025-3057
    - ⏱ 2023-02-24 20:03:12 

- 📌 分配区域的大小不会被用户传入，必须由内存分配库本身记录追踪。 ^21-3712-3742
    - ⏱ 2023-02-24 20:03:12 

- 📌 realloc()创建一个新的更大的内存区域，将旧区域复制到其中，并返回新区域的指针。 ^21-8682-8725
    - ⏱ 2023-02-24 20:03:12 
## 第15章 机制：地址转换


- 📌 LDE背后的想法很简单：让程序运行的大部分指令直接访问硬件，只在一些关键点（如进程发起系统调用或发生时钟中断）由操作系统介入来确保“在正确的时间，正确的地点，做正确的事” ^22-436-521
    - ⏱ 2023-02-24 20:04:14 

- 📌 利用地址转换，硬件对每次内存访问进行处理（即指令获取、数据读取或写入），将指令中的虚拟（virtual）地址转换为数据实际存储的物理（physical）地址。 ^22-1166-1245
    - ⏱ 2023-02-25 13:37:54 

- 📌 仅仅依靠硬件不足以实现虚拟内存，因为它只是提供了底层机制来提高效率。 ^22-1307-1341
    - ⏱ 2023-02-25 14:04:32 

- 📌 采用这种方式，在编写和编译程序时假设地址空间从零开始。但是，当程序真正执行时，操作系统会决定其在物理内存中的实际加载地址，并将起始地址记录在基址寄存器中。在上面的例子中，操作系统决定加载在物理地址32KB的进程，因此将基址寄存器设置为这个值。 ^22-4565-4686
    - ⏱ 2023-02-25 14:14:56 

- 📌 大多数情况下，操作系统正确设置硬件后，就任凭进程直接运行在CPU上，只有进程行为不端时才介入。 ^22-10293-10340
    - ⏱ 2023-02-25 14:33:24 
## 第16章 分段


- 📌 内存紧凑成本很高，因为拷贝段是内存密集型的，一般会占用大量的处理器时间。 ^23-7848-7884
    - ⏱ 2023-02-25 15:05:28 
## 第17章 空闲空间管理


- 📌 该库管理的空间由于历史原因被称为堆，在堆上管理空闲空间的数据结构通常称为空闲列表（free list）。该结构包含了管理内存区域中所有空闲块的引用。当然，该数据结构不一定真的是列表，而只是某种可以追踪空闲空间的数据结构。 ^24-1754-1864
    - ⏱ 2023-02-25 15:10:24 

- 📌 该头块中至少包含所分配空间的大小（这个例子中是20）。它也可能包含一些额外的指针来加速空间释放，包含一个幻数来提供完整性检查，以及其他信息。我们假定，一个简单的头块包含了分配空间的大小和一个幻数： ^24-5051-5149
    - ⏱ 2023-02-25 15:23:17 

- 📌 实际释放的是头块大小加上分配给用户的空间的大小。因此，如果用户请求N字节的内存，库不是寻找大小为N的空闲块，而是寻找N加上头块大小的空闲块。 ^24-5979-6079
    - ⏱ 2023-02-25 15:26:52 

- 📌 大多数传统的分配程序会从很小的堆开始，当空间耗尽时，再向操作系统申请更大的空间。通常，这意味着它们进行了某种系统调用（例如，大多数UNIX系统中的sbrk），让堆增长。操作系统在执行sbrk系统调用时，会找到空闲的物理内存页，将它们映射到请求进程的地址空间中去，并返回新的堆的末尾地址。这时，就有了更大的堆，请求就可以成功满足。 ^24-9400-9564
    - ⏱ 2023-02-25 15:48:41 

- 📌 最优匹配背后的想法很简单：选择最接近用户请求大小的块，从而尽量避免空间浪费。然而，这有代价。简单的实现在遍历查找正确的空闲块时，要付出较高的性能代价。 ^24-10061-10136
    - ⏱ 2023-02-25 15:49:04 

- 📌 最差匹配（worst fit）方法与最优匹配相反，它尝试找最大的空闲块，分割并满足用户需求后，将剩余的块（很大）加入空闲列表。最差匹配尝试在空闲列表中保留较大的块，而不是向最优匹配那样可能剩下很多难以利用的小块。 ^24-10212-10318
    - ⏱ 2023-02-25 15:49:15 

- 📌 它的表现非常差，导致过量的碎片，同时还有很高的开销。 ^24-10351-10377
    - ⏱ 2023-02-25 15:49:24 

- 📌 首次匹配有速度优势（不需要遍历所有空闲块），但有时会让空闲列表开头的部分有很多小块。因此，分配程序如何管理空闲列表的顺序就变得很重要。一种方式是基于地址排序（address-based ordering）。通过保持空闲块按内存地址有序，合并操作会很容易，从而减少了内存碎片。 ^24-10525-10662
    - ⏱ 2023-02-25 15:49:28 

- 📌 分离空闲列表一直以来有一种很有趣的方式叫作分离空闲列表（segregated list）。基本想法很简单：如果某个应用程序经常申请一种（或几种）大小的内存空间，那就用一个独立的列表，只管理这样大小的对象。其他大小的请求都交给更通用的内存分配程序。 ^24-12099-12236
    - ⏱ 2023-02-25 15:49:59 
## 第18章 分页：介绍


- 📌 可能最大的改进就是灵活性：通过完善的分页方法，操作系统能够高效地提供地址空间的抽象，不管进程如何使用地址空间。 ^25-1538-1593
    - ⏱ 2023-03-15 20:11:36 

- 📌 另一个优点是分页提供的空闲空间管理的简单性 ^25-1633-1654
    - ⏱ 2023-03-15 20:11:44 

- 📌 页表是每个进程一个（per-process）的数据结构 ^25-2317-2344
    - ⏱ 2023-03-15 20:13:15 

- 📌 [插图] ^25-4217-4218
    - ⏱ 2023-03-15 20:17:04 

- 📌 页表可以存储在操作系统的虚拟内存中（甚至可以交换到磁盘上） ^25-4998-5027
    - ⏱ 2023-03-15 20:20:32 

- 📌 最简单的形式称为线性页表（linear page table），就是一个数组。操作系统通过虚拟页号（VPN）检索该数组，并在该索引处查找页表项（PTE），以便找到期望的物理帧号（PFN） ^25-5472-5565
    - ⏱ 2023-03-15 20:22:23 
## 第19章 分页：快速地址转换（TLB）


- 📌 TLB带来了巨大的性能提升，实际上，因此它使得虚拟内存成为可能[C95]。 ^26-974-1011
    - ⏱ 2023-03-15 20:43:04 

- 📌 典型页的大小一般为4KB，这种情况下，密集的、基于数组的访问会实现极好的TLB性能，每页的访问只会遇到一次未命中。 ^26-4376-4433
    - ⏱ 2023-03-15 20:50:01 

- 📌 CISC背后的思想是，指令应该是高级原语，这让汇编语言本身更易于使用，代码更紧凑。 ^26-7751-7792
    - ⏱ 2023-03-15 20:57:09 

- 📌 RISC 背后的关键观点是，指令集实际上是编译器的最终目标，所有编译器实际上需要少量简单的原语，可以用于生成高性能的代码。 ^26-7820-7881
    - ⏱ 2023-03-15 20:57:16 

- 📌 如果一个程序短时间内访问的页数超过了TLB中的页数，就会产生大量的TLB未命中，运行速度就会变慢。这种现象被称为超出TLB覆盖范围（TLB coverage） ^26-14182-14261
    - ⏱ 2023-03-15 21:05:38 
## 第21章 超越物理内存：机制


- 📌 当操作系统发现有少于LW个页可用时，后台负责释放内存的线程会开始运行，直到有HW个可用的物理页。 ^28-8073-8121
    - ⏱ 2023-03-15 21:56:44 
## 第22章 超越物理内存：策略


- 📌 LRU具有所谓的栈特性（stack property）[M+70]。对于具有这个性质的算法，大小为N + 1的缓存自然包括大小为N的缓存的内容。 ^29-6166-6256
    - ⏱ 2023-03-15 22:14:25 

- 📌 从计算开销的角度来看，近似LRU更为可行，实际上这也是许多现代系统的做法。 ^29-12275-12312
    - ⏱ 2023-03-15 22:19:27 
## 第24章 内存虚拟化总结对话


- 📌 我们从简单的结构（如数组，即线性页表）开始，一直到多级表（它们看起来像树），甚至像内核虚拟内存中的可分页页表一样疯狂。全是为了在内存中节省一点空间！ ^31-2042-2116
    - ⏱ 2023-03-15 22:32:05 
## 第26章 并发：介绍


- 📌 线程有一个程序计数器（PC），记录程序从哪里获取指令。每个线程有自己的一组用于计算的寄存器。 ^33-781-827
    - ⏱ 2022-03-07 14:08:12 

- 📌 与进程相比，线程之间的上下文切换有一点主要区别：地址空间保持不变（即不需要切换当前使用的页表） ^33-1024-1071
    - ⏱ 2023-03-15 22:39:30 

- 📌 临界区（critical section）是访问共享资源的一段代码，资源通常是一个变量或数据结构。 ^33-10168-10217
    - ⏱ 2023-03-15 22:46:02 

- 📌 竞态条件（race condition）出现在多个执行线程大致同时进入临界区时，它们都试图更新共享的数据结构，导致了令人惊讶的（也许是不希望的）结果。 ^33-10230-10305
    - ⏱ 2023-03-15 22:46:06 

- 📌 不确定性（indeterminate）程序由一个或多个竞态条件组成，程序的输出因运行而异，具体取决于哪些线程在何时运行。 ^33-10318-10378
    - ⏱ 2023-03-15 22:46:16 
# 读书笔记

## 第4章 抽象：进程

### 划线评论
- 📌 一旦进程被阻塞（例如，通过发起I/O操作），OS将保持进程的这种状态，直到发生某种事件（例如，I/O完成）  ^360415329-7Gf4RtZed
    - 💭 运行的时候，使用CPU执行指令；阻塞的时候，不使用CPU执行指令。
以IO操作为例：
发起IO之后，需要等待IO操作的完成才能进行后续（使用CPU）的操作，在等待期间是不使用CPU的，又因为IO速度远不及CPU的速度，所以一定会有一段时间不使用CPU，我们把"等待IO完成而暂停使用CPU"的状态称为阻塞态。
    - ⏱ 2023-02-23 17:36:51

### 划线评论
- 📌 具体来说，它会将参数填入main()函数，即argc和argv数组。  ^360415329-7GeYAj1HZ
    - 💭 argc（argument count），表示argv这个数组中保存的命令行参数的个数。
argv（argument vector），表示传入main函数的参数序列，并且第一个参数argv[0]一定是程序的全路径名称，所以需要输入的main函数的参数个数是argc-1个。
    - ⏱ 2023-02-23 16:01:00

### 划线评论
- 📌 将高级策略与其低级机制分开  ^360415329-7GeX2c1AV
    - 💭 机制是底层的，策略是顶层的。
    - ⏱ 2023-02-23 15:37:20
   
## 第8章 调度：多级反馈队列

### 划线评论
- 📌 因此，MLFQ调度策略的关键在于如何设置优先级。MLFQ没有为每个工作指定不变的优先情绪而已，而是根据观察到的行为调整它的优先级。  ^360415329-7GhylP8fa
    - 💭 这里原著中是“Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior. ”译本中“优先情绪而已”应该是翻译的老师打错字了...
    - ⏱ 2023-02-25 08:40:27
   
# 本书评论
