
## 源文本文件解析

[[../../专业/软件/QQ使用技巧/QQ消息记录导出|如何导出消息记录可以看这里]]

```text
消息记录（此消息记录为文本格式，不支持重新导入）

================================================================
消息分组:XXX
================================================================
消息对象:XXX
================================================================

```

消息记录中前8行是元信息，标注了好友所在的分组和消息对象（注意，这里的对象显示的是备注名称）

QQ消息记录的格式为：

```text
[yyyy-MM-dd HH:mm:ss] [备注]
[消息内容]
[空行]
```

其中备注名为在当前那个时间段此人的备注信息，并且自己的名字也是

比如：

```
2022-01-01 14:59:09 昵称1-1
测试

2022-01-01 14:59:09 昵称2-1
测试

2022-02-12 14:59:11 昵称2-2
测试

2022-02-12 14:59:11 昵称1-2
测试

```

自己改了名字，并且对方改了名字，消息记录中的名称就全都不一样了

注意：
- 消息记录中的消息并不是压缩成一行的，换行符也会体现在消息记录中。

## 文本预处理

### 去除前8行的元信息

可以手动删除，也可以直接使用Python readlines然后写入`lines[8:]`

得到文件`backup01.txt`

### 分离时间戳和消息文本

使用正则表达式校验文本的一行是否是时间戳，然后进行分离，并分别写入两个文件

相关正则表达式可以看这里[[../../专业/正则表达式/正则表达式-解决方案/【正则表达式】校验时间和日期格式| 校验时间和日期格式]]

这一步得到的是所有的时间戳，按照行排列；所有的消息内容，去掉换行符变成一行

所有的消息序列

![[assets/Pasted image 20230125220302.png]]

所有的消息内容，变成一行

![[assets/Pasted image 20230125220600.png]]

关键代码：

```python
  with open('./asssets/text/backup01.txt', 'r', encoding='utf-8') as f:
    for line in f:
      if re.search(REGEX_IS_DATETIME, line) is None and line != '\n':
        backup02_text += line
      if re.search(REGEX_IS_DATETIME, line) is not None:
        timeline01 += line[0:line.find(' ', 18)] + '\n'

  fw = open('./asssets/text/backup02.txt', 'w', encoding='utf-8')
  fw.write(backup02_text.replace('\n', ''))
  fw = open('./asssets/text/timeline01.txt', 'w', encoding='utf-8')
  fw.write(timeline01)
```

得到文件`backup02.txt`和`timeline.txt`

### 去除各种符号和数字，只保留汉字

去除所有的表情，图片，英文和空格

```python
  file = open('./asssets/text/backup03.txt', 'r', encoding='utf-8').read()

  # 去除符号
  r = r'[，？。\%、；1234567890.n-】【“”《》,（）！()]'
  file =re.sub(r, '', file)
  # 去除表情
  r = r'\[图片\]'
  file =re.sub(r, '', file)
  # 去除图片
  r = r'\[表情\]'
  file =re.sub(r, '', file)
  # 去除字母
  r = r'[a-zA-Z]'
  file =re.sub(r, '', file)
  # 去除空格
  r = r' '
  file =re.sub(r, '', file)

  fw = open('./asssets/text/backup04.txt', 'w', encoding='utf-8')
  fw.write(file)
```

便于使用分词库进行分词

得到文件`backup03.txt`

## 词频分析

### 分词

安装jieba库中遇见了包安装时候的环境问题 [[../环境问题相关/Python第三方库安装相关| Python环境配置]]

相关的使用教程在[[../../基础/bigdata/python/python-libs/jieba/python-jieba|Python-jieba库]]

分词后要去掉单个字的元组，只保留有多个字的元组

分词关键代码：

```python
  # 使用用户自定义停用词表
  jieba.load_userdict("./asssets/text/dict.txt")
  # 分词
  words_cut = jieba.cut(file)
    # 词频统计
  wordcount = Counter(words_cut)
  words_dict = {}
  # 去掉单个字的元组，保留多个字的元组
  for k,v in dict(wordcount).items():
    if len(k) > 1:
      words_dict[k] = v
  wordcount = Counter(words_dict)
  # top50的词语
  data['most_common_words'] = wordcount.most_common(most_common_words)
```

### 分析

时间序列文件的行数即为发消息的总数 `timeline.txt`

```python
len(open('./asssets/text/timeline.txt', 'r', encoding='utf-8').readlines())
```

分离出来的文本的长度即为总字数（带有标点）`backup02.txt`

```python
len(open('./asssets/text/backup03.txt', 'r', encoding='utf-8').readlines()[0])
```

特定词的个数在`backup03.txt`中用正则表达式匹配即可

```python
len(re.findall(REGEX_IS_WORD_1, content_backup03))
```

## 时间分析

每一项时间序列为 `2022-01-01 14:44:48`

使用`split`可以将每一项都分离开，使用数组下标引用可以分离出所有的年月日，时分秒。

得到一年中每天的记录条数，一年中每一个月的记录数量，每个小时的数量

使用[[../../基础/bigdata/python/python-libs/collections/【Python库】collections.Conter|Python的Counter类]]可以方便的进行计数

对于一天中24小时的数据，计数之后将三小时分为一组进行汇总

```python
  # 每3小时为一组，分组汇总聊天记录条数
  message_time_grouped = {"0":0, "1":0, "2":0, "3":0, "4":0, "5":0, "6": 0, "7":0}
  for key, value in dict(counter_time).items():
    message_time_grouped[str(math.floor(int(key) / 3))] += value
  data['message_time_grouped'] = [{"value": value, "name":key} for key, value in message_time_grouped.items()]

```

其中0代表0点到3点，1代表3点到6点，以此类推

注意，这里如果有一天的消息没有，就要置为0, 以下是主要代码

```python
  # 如果某天没有数据，那么要用0占位
  dict_counter_date_day = dict(counter_date_day)
  month_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30 ,31]
  for i in range(1, 13):
    for j in range(1, month_days[i - 1] + 1):
      date = '2022-0' + str(i) if i < 10 else '2022-' + str(i)
      date += '-0' + str(j) if j < 10 else '-' + str(j)
      if date not in dict_counter_date_day:
        dict_counter_date_day[date] = 0
  counter_date_day = Counter(sorted(dict_counter_date_day.items(), key = lambda v:v[0]))
```

---

## Python代码汇总

```python
import re
import jieba
import json
import math

from collections import Counter

# 最终结果
data = {}

# 静态变量

REGEX_IS_DATETIME = '((([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29)) ([0-1]?[0-9]|2[0-3]):([0-5][0-9]):([0-5][0-9])'

REGEX_IS_DATE = '((([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29))'

REGEX_IS_TIME = '([0-1]?[0-9]|2[0-3]):([0-5][0-9]):([0-5][0-9])'

REGEX_IS_EMOJI = r'\[表情\]'

REGEX_IS_PICTURE = r'\[图片\]'

REGEX_IS_WORD_1 = r'晚安'

REGEX_IS_WORD_2 = r'哈+'

REGEX_IS_WORD_3 = r'em+'

MOST_COMMON_WORDS = 80

# ------------------------------------------------------------------
# step1：文本提取
#  1. 将原本的聊天记录文本去掉所有空行和时间戳，得到 backup02_text
#  2. 提取所有的时间戳
# ------------------------------------------------------------------
def step1():

  # backup02_text 是去掉所有空行和时间戳的原文本
  backup02_text = ''
  # timeline是提取了所有的时间戳
  timeline01 = ''

  with open('./asssets/text/backup01.txt', 'r', encoding='utf-8') as f:
    for line in f:
      if re.search(REGEX_IS_DATETIME, line) is None and line != '\n':
        backup02_text += line
      if re.search(REGEX_IS_DATETIME, line) is not None:
        timeline01 += line[0:line.find(' ', 18)] + '\n'

  fw = open('./asssets/text/backup02.txt', 'w', encoding='utf-8')
  fw.write(backup02_text.replace('\n', ''))
  fw = open('./asssets/text/timeline.txt', 'w', encoding='utf-8')
  fw.write(timeline01)

# ------------------------------------------------------------------
# step2: 去除各种符号和数字
# 1. 只保留单纯的文本
# ------------------------------------------------------------------
def step2():
  file = open('./asssets/text/backup02.txt', 'r', encoding='utf-8').read()

  # 去除符号
  r = r'[，？。\%、；1234567890.n-】【“”《》,（）！()]'
  file =re.sub(r, '', file)
  # 去除表情
  r = r'\[图片\]'
  file =re.sub(r, '', file)
  # 去除图片
  r = r'\[表情\]'
  file =re.sub(r, '', file)
  # 去除字母
  r = r'[a-zA-Z]'
  file =re.sub(r, '', file)
  # 去除空格
  r = r' '
  file =re.sub(r, '', file)

  fw = open('./asssets/text/backup03.txt', 'w', encoding='utf-8')
  fw.write(file)

# ------------------------------------------------------------------
# 分词
# 分词，并去掉所有单个的无意义的字，只保留前most_common_words的词语
# ------------------------------------------------------------------
def split_words():
  file = open('./asssets/text/backup03.txt', 'r', encoding='utf-8').read()
  jieba.load_userdict("./asssets/text/dict.txt")
  words_cut = jieba.cut(file)
  # 词频统计
  wordcount = Counter(words_cut)
  words_dict = {}
  # 去掉单个字的元组，保留多个字的元组
  for k,v in dict(wordcount).items():
    if len(k) > 1:
      words_dict[k] = v
  wordcount = Counter(words_dict)
  # top50的词语, most_common 返回的是一个元组列表
  list_most_common_words = wordcount.most_common(MOST_COMMON_WORDS)

  # 常用词的词云图添加到data中
  data['most_common_words'] = [{'name': item[0], 'value': item[1]} for item in list_most_common_words]


# ------------------------------------------------------------------
# 统计字数相关
# 1. 发了多少条消息
# 2. 总字数
# 3. 图片个数
# 4，表情个数
# 5. 晚安、emm、哈哈哈等词的个数
# ------------------------------------------------------------------
def statistic_words():
  timeline = open('./asssets/text/timeline.txt', 'r', encoding='utf-8').readlines()
  content_backup03 = open('./asssets/text/backup03.txt', 'r', encoding='utf-8').readlines()[0]

  # * 一共发了多少条消息
  count_message = len(timeline)
  data['count_message'] = count_message
  log('一共发送了 ' + str(count_message) + ' 条消息')

  # * 总字数
  count_words = len(content_backup03)
  data['count_words'] = count_words
  log('总字数 ' + str(count_words))

  # * 图片个数
  count_emoji = len(re.findall(REGEX_IS_EMOJI, content_backup03))
  data['count_emoji'] = count_emoji
  log('图片个数 '+ str(count_emoji))

  # * 表情个数
  count_picture = len(re.findall(REGEX_IS_PICTURE, content_backup03))
  data['count_picture'] = count_picture
  log('表情个数 ' + str(count_picture))

  # * 特定词的个数
  data_words = {}

  # * - 晚安
  data_words['晚安'] = len(re.findall(REGEX_IS_WORD_1, content_backup03))
  log('特定词-晚安 ' + str(data_words['晚安']))

  # * - 哈哈...
  data_words['晚安'] = len(re.findall(REGEX_IS_WORD_2, content_backup03))
  log('特定词-哈哈 ' + str(data_words['晚安']))

  data['words'] = data_words

# ------------------------------------------------------------------
# 统计时间相关
# 0. 一年之中的聊天数据图（精确到每一天）
# 1. * 一年之中聊天最多的日期
# 2. 一年之中的聊天数据图（精确到每一月）
# 3. * 一年之中聊天最多的月份
# 4. 一天之中聊天数据图
# 5. * 一天之中聊天最多的时间
# ------------------------------------------------------------------
def statistic_time():
  timeline01 = open('./asssets/text/timeline.txt', 'r', encoding='utf-8').readlines()
  timeline_date_day = []
  timeline_date_month = []
  timeline_time = []
  for line in timeline01:
    timeline_date_day.append(line.split(' ')[0])
    timeline_date_month.append(line.split(' ')[0].split('-')[1])
    timeline_time.append(line.split(' ')[1].split(':')[0])
  counter_date_day = Counter(timeline_date_day)
  counter_date_month = Counter(timeline_date_month)
  counter_time = Counter(timeline_time)

  # 每3小时为一组，分组汇总聊天记录条数
  message_time_grouped = {"0":0, "1":0, "2":0, "3":0, "4":0, "5":0, "6": 0, "7":0}
  for key, value in dict(counter_time).items():
    message_time_grouped[str(math.floor(int(key) / 3))] += value
  data['message_time_grouped'] = [{"value": value, "name":key} for key, value in message_time_grouped.items()]
  log(data['message_time_grouped'])

  # 如果某天没有数据，那么要用0占位
  data['chat_days'] = len(counter_date_day)
  dict_counter_date_day = dict(counter_date_day)
  month_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30 ,31]
  for i in range(1, 13):
    for j in range(1, month_days[i - 1] + 1):
      date = '2022-0' + str(i) if i < 10 else '2022-' + str(i)
      date += '-0' + str(j) if j < 10 else '-' + str(j)
      if date not in dict_counter_date_day:
        dict_counter_date_day[date] = 0
  counter_date_day = Counter(sorted(dict_counter_date_day.items(), key = lambda v:v[0]))

  # 一年中每天发送消息的数量
  data['message_year_day'] = [item for item in dict(counter_date_day)]

  # 一年中每个月发送消息的数量
  data['message_year_month'] = {
    "xData": [key for key, value in dict(counter_date_month).items()], 
    "yData": [value for key, value in dict(counter_date_month).items()]
  }

  # 一天中每一个小时发送消息的数量
  dict_counter_time = sorted(dict(counter_time).items(), key = lambda v:int(v[0]))
  data['message_time'] = {
    "xData": [v[0] for v in dict_counter_time], 
    "yData": [v[1] for v in dict_counter_time], 
  }

  data['message_max_day'] = counter_date_day.most_common(1)
  data['message_max_month'] = counter_date_month.most_common(1)
  data['message_max_time'] = counter_time.most_common(1)

# ------------------------------------------------------------------
# 创建分析后的echarts_json文件
# ------------------------------------------------------------------
def create_json():
  if data == {}:
    step1()
    step2()
    # split_words()
    statistic_words()
    statistic_time()

  open('./deploy/data.json', 'w').write(json.dumps(data))
  log("JSON文件创建完成")

def log(string):
  print("[log]" + str(string))

if __name__ == '__main__':
  create_json()

```

